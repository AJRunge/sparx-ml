# Configuration for the KDD CUP 99 model generation
app.kdd : {
  # The prefix should be set for each app type (e.g 'kdd' for KDD and 'nab' for NAB)
  prefix = kdd
  # The path holding the temporary or "work in progress" files
  path.wip = /tmp/sparx-ml/kdd/wip
  # The root output path for the generated model, predictions...
  path.output = /tmp/sparx-ml/kdd/out
  # Column name of the dataframe column containing the raw data to be processed. This should not be changed.
  raw.input.col.name = "RAW_INPUT_LINE"
  # Column name of the dataframe column containing the timestamp of the data input. This should not be changed.
  raw.timestamp.col.name = "TIMESTAMP"
  # Column name of the column of vectors that will be used for model generation / prediction. This should not be changed.
  prepared.data.col.name = "PREPARED_DATA"
  # Section covering training parameters in details
  training {
    # Training data split ratio (a number between 0 and 1, representing the percentage of data
    # that will be used for actual training (not cross validation nor test)
    split.ratio = 0.7
    # Random seed used for splitting the data
    split.seed = 7774777
    # File used for training the model
    file = ./src/test/resources/kdd/kddcup.data_01_percent
    # Schema description of the transformed input file
    # There are multiple ways to build this, but this can be used a sample (see the org.apache.spark.sql.types.StructType)
    input.schema = {"type":"struct","fields":[
      {"name":"duration","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"protocol_type","type":"string","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"service","type":"string","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"flag","type":"string","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"src_bytes","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_bytes","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"land","type":"integer","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"wrong_fragment","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"urgent","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"hot","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"num_failed_logins","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"logged_in","type":"integer","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"num_compromised","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"root_shell","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"su_attempted","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"num_root","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"num_file_creations","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"num_shells","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"num_access_files","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"num_outbound_cmds","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"is_host_login","type":"integer","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"is_guest_login","type":"integer","nullable":true,"metadata":{"continuous":false,"label":false}},
      {"name":"count","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"srv_count","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"serror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"srv_serror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"rerror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"srv_rerror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"same_srv_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"diff_srv_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"srv_diff_host_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_count","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_srv_count","type":"integer","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_same_srv_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_diff_srv_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_same_src_port_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_srv_diff_host_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_serror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_srv_serror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_rerror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"dst_host_srv_rerror_rate","type":"double","nullable":true,"metadata":{"continuous":true,"label":false}},
      {"name":"label","type":"string","nullable":true,"metadata":{"continuous":false,"label":true}}
    ]}
    # Configuration parametrs for the kmeans model
    kmeans : {
      # Clusters or cluster range
      # Acceptable values are:
      #  - a single int (e.g. 140)
      #  - an inclusive range, defined as folowing: from, to, step (e.g. 140, 150, 5 will produce a sequence containing 140, 145 and 150)
      clusters = "140, 150, 10"
      # Iterations or iterantions range
      # See clusters for acceptable values
      iterations = "100"
      # Tolerance or tolerances range
      # See clusters for acceptable values
      tolerances = "10"
      # Folds represents the number of time that the cross validation will run the algorithm with the same parameters, choosing the best out of them
      folds = "3"
    }
  }
  # Section covering prediction parameters in details
  prediction : {
    # Threshold that delimits normal records from anomalies. Anomalies have a probability smaller than this threshold.
    threshold = 0.003
    # Pipeline used for predicting data; this should be the pipeline built by TranXKMeans runnable
    pipeline = /tmp/sparx-ml/kdd/out/kdd_pipeline_........plm
    # File to run predictions on
    file = ./src/test/resources/kdd/kddcup.newtestdata_01_percent_unlabeled
    # file = ./src/test/resources/kdd/kddcup.data_01_percent
  }
}

# Configuration template proposal for the NAB model generation.
# This should be removed if not required by the NAB team.
app.nab : {
  # The prefix should be set for each app type (e.g 'kdd' for KDD and 'nab' for NAB)
  prefix = nab
  # The path holding the temporary or "work in progress" files
  path.wip = /tmp/sparx-ml/nab/wip
  # The root output path for the generated model, predictions...
  path.output = /tmp/sparx-ml/nab/out
  # Column name of the dataframe column containing the raw data to be processed
  raw.input.col.name = "RAW_INPUT_LINE"
  # Column name of the dataframe column containing the timestamp of the data input
  raw.timestamp.col.name = "TIMESTAMP"
  # Column name of the column of vectors that will be used for model generation / prediction
  prepared.data.col.name = "PREPARED_DATA"
  # Section covering training parameters in details
  training {
    # Training data split ratio (a number between 0 and 1, representing the percentage of data
    # that will be used for actual training (not cross validation nor test)
    split.ratio = 0.8
    # Random seed used for splitting the data
    split.seed = 7774777
    # File used for training the model
    file = ./src/test/resources/nab/art_daily_small_noise.csv
    # Schema description of the transformed input file
    # There are multiple ways to build this, but this can be used a sample (see the org.apache.spark.sql.types.StructType)
    input.schema = {
      "type": "struct", "fields": [
        {"name": "timestamp", "type": "timestamp", "nullable": true, "metadata": {"continuous": true, "label": false}},
        {"name": "value", "type": "double", "nullable": true, "metadata": {"continuous": true, "label": false}}
      ]
    }
    # Configuration parametrs for the kmeans model
    kmeans : {
      # Clusters or cluster range
      # Acceptable values are:
      #  - a single int (e.g. 140)
      #  - an inclusive range, defined as folowing: from, to, step (e.g. 140, 150, 5 will produce a sequence containing 140, 145 and 150)
      clusters = "32, 48, 2"
      # Iterations or iterations range
      # See clusters for acceptable values
      iterations = "40"
      # Tolerance or tolerances range
      # See clusters for acceptable values
      tolerances = "10"
      # Folds represents the number of time that the cross validation will run the algorithm with the same parameters, choosing the best out of them
      folds = "3"
    }
  }
  # Section covering prediction parameters in details
  prediction : {
    # Threshold that delimits normal records from anomalies. Anomalies have a probability smaller than this threshold.
    threshold = 0.001
    # Pipeline used for predicting data; this should be the pipeline built by TranXKMeans runnable.
    pipeline = /tmp/sparx-ml/nab/out/nab_pipeline........plm
    # File to run predictions on
    file = ./src/test/resources/nab/art_daily_flatmiddle.csv
    #file = ./src/test/resources/nab/art_daily_small_noise.csv
  }
}

# Configuration for the prediction stream
app.streaming : {

  # The prefix should be set for each app type (e.g 'kdd' for KDD and 'nab' for NAB)
  prefix = kdd

  # Coma separated value list of pipelines to be used for predictions
  prediction.pipelines = /tmp/sparx-ml/kdd/out/kdd_pipeline_............plm

  # Stream batch interval
  stream.batch.duration.seconds = 20

  # For the file streamm the folder where the files are expected
  stream.file.directory = /tmp/sparx-ml/kdd/in/test

  # Predifined index root
  es.index.root = sparx-ml

  # The raw.input.col.name param should match the same param from modgen
  raw.input.col.name = "RAW_INPUT_LINE"
  # The raw.timestamp.col.name param should match the same param from modgen
  raw.timestamp.col.name = "TIMESTAMP"

  # Kafka stream configuration
  # The user should define this parameters
  #stream.kafka : {
  #  brokers = "kafka_host:9092"
  #  topics = "kdd-topic"
  #}

}


